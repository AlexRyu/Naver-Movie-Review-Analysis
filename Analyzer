# Splitting lines of a raw text file
def read_data(filename):
   with open(filename, 'r') as f:
       data = [line.split('\t') for line in f.read().splitlines()]
       data = data[1:] # header ƃŗ
   return data

train_data = read_data('ratings_train.txt')
test_data = read_data('ratings_test.txt')

print(len(train_data)) # nrows: 150000
print(len(train_data[0])) # ncols: 3
print(len(test_data)) # nrows: 50000
print(len(test_data[0])) # ncols: 3


from konlpy.tag import Twitter

pos_tagger = Twitter()
def tokenize(doc):
    norm, stemŦ optional
    return ['/'.join(t) for t in pos_tagger.pos(doc, norm=True, stem=True)]
train_docs = [(tokenize(row[1]), row[2]) for row in train_data]
test_docs = [(tokenize(row[1]), row[2]) for row in test_data]

from pprint import pprint
pprint(train_docs[0]) # Result

# Checking the number of tokens we have
tokens = [t for d in train_docs for t in d[0]]
print(len(tokens))

import nltk
text = nltk.Text(tokens, name='NMSC')
print(text)

print(len(text.tokens)) # returns number of tokens
# => 2194536
print(len(set(text.tokens))) # returns number of unique tokens
# => 48765
pprint(text.vocab().most_common(10)) # returns frequency distribution

text.plot(50) # Plot sorted frequency of top 50 tokens

# Error pops up due to font issues
from matplotlib import font_manager, rc
font_fname = 'c:/windows/fonts/gulim.ttc' # A font of your choice
font_name = font_manager.FontProperties(fname=font_fname).get_name()
rc('font', family=font_name)

text.plot(50) # Plot sorted frequency of top 50 tokens 

text.collocations()

# Sentiment classification with term-existance
selected_words = [f[0] for f in text.vocab().most_common(2000)]
def term_exists(doc):
   return {'exists({})'.format(word): (word in set(doc)) for word in selected_words}

# For tutorial purpose, limiting the size
train_docs = train_docs[:10000]
train_xy = [(term_exists(d), c) for d, c in train_docs]
test_xy = [(term_exists(d), c) for d, c in test_docs]


# Sentiment classification with term-existance ==> Using Naive Bayes classifier
classifier = nltk.NaiveBayesClassifier.train(train_xy)
print(nltk.classify.accuracy(classifier, test_xy))
# => 0.80418
classifier.show_most_informative_features(10)


# Sentiment classification with doc2vec (feat. Gensim)
from collections import namedtuple
TaggedDocument = namedtuple('TaggedDocument', 'words tags')
# Here, we use all 15k review data
tagged_train_docs = [TaggedDocument(d, [c]) for d, c in train_docs]
tagged_test_docs = [TaggedDocument(d, [c]) for d, c in test_docs]


from gensim.models import doc2vec
# Buliding Dictionary
doc_vectorizer = doc2vec.Doc2Vec(size=300, alpha=0.025, min_alpha=0.025, seed=1234)
doc_vectorizer.build_vocab(tagged_train_docs)
# Train document vectors!
for epoch in range(10):
  doc_vectorizer.train(tagged_train_docs)
  doc_vectorizer.alpha -= 0.002 # decrease the learning rate
  doc_vectorizer.min_alpha = doc_vectorizer.alpha # fix the learning rate, no decay
# To save
# doc_vectorizer.save('doc2vec.model')


# Testing the model
pprint(doc_vectorizer.most_similar('공포/Noun'))
pprint(doc_vectorizer.most_similar(positive=['여자/Noun', '왕/Noun'], negative=['남자/Noun']))
text.concordance('왕/Noun', lines=10)

# An error exist because it cannot distinguish various meaning in a word with multi meanings
# The last step of classification ==> Lets make feautres for calssifications
train_x = [doc_vectorizer.infer_vector(doc.words) for doc in tagged_train_docs]
train_y = [doc.tags[0] for doc in tagged_train_docs]
len(train_x) # This might be a reason to why this is not a fair comparison with the prior term existence
# => 150000
len(train_x[0])
# => 300
test_x = [doc_vectorizer.infer_vector(doc.words) for doc in tagged_test_docs]
test_y = [doc.tags[0] for doc in tagged_test_docs]
len(test_x)
# => 50000
len(test_x[0])
# => 300


from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state=1234)
classifier.fit(train_x, train_y)
classifier.score(test_x, test_y)
#=>0.78 / Accuracy is about 78%. Not bad for a first trial.
